# Project 4 Predicting Higher Education Students' Performance

This project uses ML to predict whether students will have a good or bad final grade on specific university classes based on personal characteristics, family characteristics, and education habits. Predicting which students would likely have a poor outcome would allow educators and advisors to target those students with extra services or opportunities to help them succeed.

The dataset is entitled "Higher Education Students Performance Evaluation" made available by the Faculty of Engineering and the Faculty of Educational Sciences students in 2019 and was acquired from the UC Irvine Machine Learing Repository (https://archive.ics.uci.edu/dataset/856/higher+education+students+performance+evaluation). This dataset is licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0) license. Citation: Yilmaz,Nevriye and Şekeroğlu,Boran. (2023). Higher Education Students Performance Evaluation. UCI Machine Learning Repository. https://doi.org/10.24432/C51G82.

Prior to preprocessing, the Data file was loaded into a PostgreSQL database. It was retrieved using sqlalchemy and loaded into a Pandas DataFrame. During preprocessing, "Machine learning: one-hot encoding vs integer encoding" by Stéphanie Crêteur,Published in Geek Culture, Dec 16, 2022, https://medium.com/geekculture/machine-learning-one-hot-encoding-vs-integer-encoding-f180eb831cf1 was consulted when deciding which categories made sense to leave integer encoded, and which to use One-Hot Encoding on. Basically, if the integer encoding makes numeric sense, such as feature 4 "Scholarship type" which is in the dataset encoded as 1: None, 2: 25%, etc., the integer encoding was left in place. However, if the encoding made no numeric sense, such as feature 3 "Graduated high-school type" which is encoded in the dataset as 1: private, 2: state, 3: other, the integer encoding was replace with One-Hot Ecoding via Pandas's get_dummies. Also, the target feature "GRADE" was mapped such that original values of 0, 1, & 2 (grades less than a C) were replaced with 0's and 3, 4, 5, 6, & 7 (grades C and higher) were replaced with 1's making this a Binary Classification Problem, so Logistic Regression was used to predict student outcomes.

The first Logistic Regression Model tried produced an accuracy score of 73%.

For ideas regarding optimizing the Logistic Regression Model, "Fine-tuning Parameters in Logistic Regression" By Saturn Cloud, July 10, 2023, https://saturncloud.io/blog/finetuning-parameters-in-logistic-regression/ was consulted. A technique called "Grid Search" was mentioned in the article, and I chose to use this technique as it seems simple and concise. The Scikitlearn documentation was also consulted to pick reasonable metrics to try to optimize the model. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html .

The results of the Grid Search are saved in the Excel file entitled "logistic_regression_results.xlsx" in this repository. The model metrics which produced the highest Accuracy Score (83.8%) were extracted from the results and then sorted on the Max Iterations column. Model metrics that differed only by Max Iterations were dropped except for the models with the fewest Max Iterations since a look at the Confusion Matrices and Classification Reports showed that there were no differences in the predictions of the various models. There were six optimal models identified and they are stored in the 'filtered_max_accuracy_df' which is shown at the bottom of the Jupyter Notebook entitled "Predicting_Student_Performance_Using_ML."

Probably the biggest challenge with this dataset is that it is so small. I would have like to use a Multinomial Logistic Regression to predict the original grades, but I was unable to produce a meaningful model with the amount of observations in the dataset. The metric that I found during Google searching is that there need to be 10 times as many observations as features, and this set has 30 features but only 145 observations.
